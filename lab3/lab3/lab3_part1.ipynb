{
 "metadata": {
  "name": "",
  "signature": "sha256:85d724e3b32404031ec0d2a817d90c9c777d07057ef021110045c7e9d4a1f24f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lab 3: Bayesian PCA (Part 1)\n",
      "\n",
      "### Machine Learning: Principles and Methods, March 2015\n",
      "\n",
      "* The lab exercises should be made in groups of two people.\n",
      "* The deadline for part 1 is Sunday, 22 March, 23:59.\n",
      "* Assignment should be sent to D.P.Kingma at uva dot nl (Durk Kingma). The subject line of your email should be \"[MLPM2015] lab3part1_lastname1\\_lastname2\". \n",
      "* Put your and your teammates' names in the body of the email\n",
      "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file follows the same rule as the subject line. For example, if the subject line is \"[MLPM2013] lab01\\_Kingma\\_Hu\", the attached file should be \"lab0\\_Kingma\\_Hu.ipynb\". Only use underscores (\"\\_\") to connect names, otherwise the files cannot be parsed.\n",
      "\n",
      "Notes on implementation:\n",
      "\n",
      "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please contact us.\n",
      "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
      "* NOTE: test your code and make sure we can run your notebook / scripts!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Introduction\n",
      "\n",
      "In this lab assignment, we will implement a variational algorithm for Bayesian PCA. Unlike regular PCA based on maximization of retained variance or minimization of projection error (see Bishop, 12.1.1 and 12.1.2), probabilistic PCA defines a proper density model over observed and latent variables. We will work with a fully Bayesian model this time, which is to say that we will put priors on our parameters and will be interested in learning the posterior over those parameters. Bayesian methods are very elegant, but require a shift in mindset: we are no longer looking for a point estimate of the parameters (as in maximum likelihood or MAP), but for a full posterior distribution.\n",
      "\n",
      "The integrals involved in a Bayesian analysis are usually analytically intractable, so that we must resort to approximations. In this lab assignment, we will implement the variational method described in Bishop99. Chapters 10 and 12 of the PRML book contain additional material that may be useful when doing this exercise.\n",
      "\n",
      "* [Bishop99] Variational Principal Components, C. Bishop, ICANN 1999 - http://research.microsoft.com/pubs/67241/bishop-vpca-icann-99.pdf\n",
      "\n",
      "Below, you will find some code to get you started."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import scipy.special as sp\n",
      "\n",
      "class BayesianPCA(object):\n",
      "    \n",
      "    def __init__(self, d, N, a_alpha=10e-3, b_alpha=10e-3, a_tau=10e-3, b_tau=10e-3, beta=10e-3):\n",
      "        \"\"\"\n",
      "        \"\"\"\n",
      "        self.d = d # number of dimensions\n",
      "        self.N = N # number of data points\n",
      "        \n",
      "        # Hyperparameters\n",
      "        self.a_alpha = a_alpha\n",
      "        self.b_alpha = b_alpha\n",
      "        self.a_tau = a_tau\n",
      "        self.b_tau = b_tau\n",
      "        self.beta = beta\n",
      "\n",
      "        # Variational parameters\n",
      "        self.means_z = np.random.randn(d, N) # called x in bishop99\n",
      "        self.sigma_z = np.random.randn(d, d)\n",
      "        self.mean_mu = np.random.randn(d, 1)\n",
      "        self.sigma_mu = np.random.randn(d, d)\n",
      "        self.means_w = np.random.randn(d, d)\n",
      "        self.sigma_w = np.random.randn(d, d)\n",
      "        self.a_alpha_tilde = np.abs(np.random.randn(1))\n",
      "        self.bs_alpha_tilde = np.abs(np.random.randn(d, 1))\n",
      "        self.a_tau_tilde = np.abs(np.random.randn(1))\n",
      "        self.b_tau_tilde = np.abs(np.random.randn(1))\n",
      "    \n",
      "    def __update_z(self, X):\n",
      "        pass\n",
      "    \n",
      "    def __update_mu(self):\n",
      "        pass\n",
      "    \n",
      "    def __update_w(self, X):\n",
      "        pass\n",
      "    \n",
      "    def __update_alpha(self):\n",
      "        pass\n",
      "\n",
      "    def __update_tau(self, X):\n",
      "        pass\n",
      "\n",
      "    def L(self, X):\n",
      "        L = 0.0\n",
      "        return L\n",
      "    \n",
      "    def fit(self, X):\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1. The Q-distribution (5 points)\n",
      "\n",
      "In variational Bayes, we introduce a distribution $Q(\\Theta)$ over parameters / latent variables in order to make inference tractable. We can think of $Q$ as being an approximation of a certain distribution. What function does $Q$ approximate, $p(D|\\Theta)$, $p(\\Theta|D)$, $p(D, \\Theta)$, $p(\\Theta)$, or $p(D)$, and how do you see that from the equation $\\ln p(D) = \\mathcal{L}(Q) + \\mathrm{KL}(Q||P)$? (Hint: see eq. 11 in Bishop99)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$Q(\\Theta)$ approximates $P(\\Theta|D)$.\n",
      "\n",
      "You can see that because in equation 11:\n",
      "\n",
      "$\\text{KL}(Q||P) = - \\int Q(\\Theta) \\text{ln} \\big \\{ \\frac{p(\\Theta|D)}{Q(\\Theta)} \\big\\} d\\Theta$\n",
      "\n",
      "The KL-divergence is defined over $Q(\\Theta)$ and $P(\\Theta|D)$, and we want to minimize this KL-divergence, thus we want to get \n",
      "\n",
      "$Q(\\Theta) = P(\\Theta|D)$, thus $Q(\\Theta)$ approximates $P(\\Theta|D)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 2. The mean-field approximation (15 points)\n",
      "\n",
      "Equation 13 from [Bishop99] is a very powerful result: assuming only that $Q(\\Theta)$ factorizes in a certain way (no assumptions on the functional form of the factors $Q_i$!), we get a set of coupled equations for the $Q_i$.\n",
      "\n",
      "However, the expression given in eq. 13 for Q_i contains a small mistake. Starting with the expression for the lower bound $\\mathcal{L}(Q)$, derive the correct expression (and include your derivation). You can proceed as follows: first, substitute the factorization of $Q$ (eq. 12) into the definition of $\\mathcal{L}(Q)$ and separate $\\mathcal{L}(Q)$ into $Q_i$-dependent and $Q_i$-independent terms. At this point, you should be able to spot the expectations $\\langle\\cdot\\rangle_{k \\neq i}$ over the other $Q$-distributions that appear in Bishop's solution (eq. 13). Now, keeping all $Q_k, k \\neq i$ fixed, maximize the expression with respect to $Q_i$. You should be able to spot the form of the optimal $ln Q_i$, from which $Q_i$ can easily be obtained."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\mathcal{L}(Q) = \\int d\\theta Q(\\theta) \\text{ln} \\big \\{ \\frac{p(D|\\theta)}{Q(\\theta)} \\big \\} $\n",
      "\n",
      "$Q(\\theta) =\\prod_i Q_i(\\theta_i)$\n",
      "\n",
      "$\\mathcal{L}(Q) = \\int d\\theta \\prod_j Q_j(\\theta_j) \\text{ln} \\big \\{ \\frac{p(D|\\theta)}{\\prod_j Q_j(\\theta_j)} \\big \\} $\n",
      "\n",
      "$\\mathcal{L}(Q) = \\int d\\theta \\prod_j Q_j(\\theta_j) \\big \\{  \\text{ln }p(D|\\theta) -  \\text{ln } \\prod_j Q_j(\\theta_j) \\big \\} $\n",
      "\n",
      "To separate $Q_i$ from $Q_{j\\not=i}$ we split the product and integral apart:\n",
      "\n",
      "$\\mathcal{L}(Q) = \\int  Q_i(\\theta_i) \\int \\prod_{j\\not=i} Q_j(\\theta_j) \\big \\{  \\text{ln }p(D|\\theta) -  \\text{ln } \\prod_j Q_j(\\theta_j) \\big \\} d\\theta_j d\\theta_i $\n",
      "\n",
      "$= \\int  Q_i(\\theta_i)  \\big \\{  \\int \\prod_{j\\not=i} Q_j(\\theta_j) \\text{ln }p(D|\\theta)d\\theta_j -  \\int \\prod_{j\\not=i} Q_j(\\theta_j)\\text{ln } \\prod_j Q_j(\\theta_j)d\\theta_j \\big \\}  d\\theta_i $\n",
      "\n",
      "$= \\int  Q_i(\\theta_i)  \\big \\{  \\int \\prod_{j\\not=i} Q_j(\\theta_j) \\text{ln }p(D|\\theta)d\\theta_j \\big \\}d\\theta_i -  \\int  Q_i(\\theta_i)\\big \\{ \\int \\prod_{j\\not=i} Q_j(\\theta_j)\\text{ln } \\prod_j Q_j(\\theta_j)d\\theta_j  \\big \\} d\\theta_i $\n",
      "\n",
      "$= \\int  Q_i(\\theta_i)  \\big \\{  \\int \\prod_{j\\not=i} Q_j(\\theta_j) \\text{ln }p(D|\\theta)d\\theta_j \\big \\}d\\theta_i -  \\int  \\big \\{  \\prod_{j} Q_j(\\theta_j)\\text{ln } \\prod_j Q_j(\\theta_j) \\big \\} d\\theta $\n",
      "\n",
      "Now we split those terms that are $Q_i$-dependent from those that are $Q_i$-independent:\n",
      "\n",
      "$ \\mathcal{L}(Q) = \\int  Q_i(\\theta_i)  \\big \\{  \\int \\prod_{j\\not=i} Q_j(\\theta_j) \\text{ln }p(D|\\theta)d\\theta_j \\big \\}d\\theta_i -  \\int  \\big \\{Q_i(\\theta_i) \\text{ln } Q_i(\\theta_i) \\int \\prod_{j\\not=i} Q_j(\\theta_j)\\text{ln } \\prod_{j\\not=i} Q_j(\\theta_j) d\\theta_j  \\big \\} d\\theta_i $\n",
      "\n",
      "We can move the terms that are $Q_i$-independent into a constant:\n",
      "\n",
      "$ \\mathcal{L}(Q) = \\int  Q_i(\\theta_i)  \\big \\{  \\int \\prod_{j\\not=i} Q_j(\\theta_j) \\text{ln }p(D,\\theta)d\\theta_j \\big \\}d\\theta_i -  \\int  \\big \\{Q_i(\\theta_i) \\text{ln } Q_i(\\theta_i) \\big \\} d\\theta_i + \\text{constant} $\n",
      "\n",
      "Now we can find the expectation over $j\\not=i$:\n",
      "\n",
      "$\\int \\prod_{j\\not=i} Q_j(\\theta_j) \\text{ln }p(D|\\theta)d\\theta_j = \\mathbb{E}_{j\\not=i}[ \\text{ln } p(D,\\theta)]$\n",
      "\n",
      "And rewrite as follows:\n",
      "\n",
      "$ \\mathcal{L}(Q) = \\int  Q_i(\\theta_i)  \\big \\{  \\mathbb{E}_{j\\not=i}[ \\text{ln } p(D,\\theta)] \\big \\}d\\theta_i -  \\int  \\big \\{Q_i(\\theta_i) \\text{ln } Q_i(\\theta_i) \\big \\} d\\theta_i + \\text{constant} $\n",
      "\n",
      "We can rewrite the expectation as an unnormalized distribution $\\text{ln } \\tilde p(D,\\theta_i)$, so we get:\n",
      "\n",
      "$ \\mathcal{L}(Q) = \\int  Q_i(\\theta_i) \\text{ln } \\tilde p(D,\\theta_i) d\\theta_i -  \\int Q_i(\\theta_i) \\text{ln } Q_i(\\theta_i) d\\theta_i + \\text{constant} $\n",
      "\n",
      "Which is the negative Kullback-Leibler divergence between $Q_i(\\theta_i)$ and $\\tilde p(D,\\theta_i)$. So if we want to maximize $\\mathcal{L}(Q)$ we need to minimize the Kullback-Leibler divergence, which is smallest when $Q_i(\\theta_i) = \\tilde p(D,\\theta_i)$. Thus, the best solution is given by\n",
      "\n",
      "$Q_i^*(\\theta_i)$ = $\\tilde p(D,\\theta_i) = \\text{exp} \\big \\{ \\mathbb{E}_{j\\not=i}[ \\text{ln } p(D,\\theta)] \\big \\}$.\n",
      "\n",
      "However, $Q_i^*(\\theta_i)$ is unnormalized, by way of the missing constant. Thus, we normalize by integrating over the expectancy of the $Q_{j\\not=i}$:\n",
      "\n",
      "$Q_i^*(\\theta_i)$ = $p(D,\\theta_i) = \\frac{\\text{exp} \\big \\{ \\mathbb{E}_{j\\not=i}[ \\text{ln } p(D,\\theta)] \\big \\} }{ \\int \\text{exp} \\big \\{ \\mathbb{E}_{j\\not=i}[ \\text{ln } p(D,\\theta) ]  \\big \\} d \\theta_i } $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3. The log-probability (10 points)\n",
      "\n",
      "Write down the log-prob of data and parameters, $\\ln p(\\mathbf{X}, \\mathbf{Z}, \\mathbf{W}, \\mathbf{\\alpha}, \\tau, \\mathbf{\\mu})$, in full detail (where $\\mathbf{X}$ are observed, $\\mathbf{Z}$ is latent; this is different from [Bishop99] who uses $\\mathbf{T}$ and $\\mathbf{X}$ respectively, but $\\mathbf{X}$ and $\\mathbf{Z}$ are consistent with the PRML book and are more common nowadays). Could we use this to assess the convergence of the variational Bayesian PCA algorithm? If yes, how? If no, why not?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following [Bishop99], a.o. equation 15, we know that:\n",
      "\n",
      "$p(X,Z,W,\\alpha,\\tau,\\mu) = \\prod^N_{n=1} p(x_n|z_n, W, \\mu, \\tau) p(Z) p(W|\\alpha) p(\\alpha) p(\\mu) p(\\tau)$\n",
      "\n",
      "$p(x_n|z_n, W, \\mu, \\tau) = \\mathcal{N}(x_n|Wz_n + \\mu, \\tau\\mathbf{I}_d)$\n",
      "\n",
      "$p(Z) = \\prod^N_{n=1} \\mathcal{N}(z_n|\\mathbf{0}, \\mathbf{I}_q)$\n",
      "\n",
      "$p(W|\\alpha) = \\prod^q_{i=1} (\\frac{\\alpha_i}{2\\pi})^{d/2} \\text{exp} \\big \\{ -\\frac{1}{2} \\alpha_i ||w_i||^2 \\big \\}$\n",
      "\n",
      "$p(\\mu) = \\mathcal{N}(\\mu|\\mathbf{0}, \\beta^{-1}\\mathbf{I})$\n",
      "\n",
      "$p(\\alpha) = \\prod^q_{i=1} \\Gamma(\\alpha_i|a_\\alpha, b_\\alpha)$\n",
      "\n",
      "$p(\\tau) = \\Gamma(\\tau|c_\\tau, d_\\tau)$\n",
      "\n",
      "Thus, we get\n",
      "\n",
      "$p(X,Z,W,\\alpha,\\tau,\\mu) = \\prod^N_{n=1} \\mathcal{N}(x_n|Wz_n + \\mu, \\tau\\mathbf{I}_d) \\mathcal{N}(z_n|\\mathbf{0}, \\mathbf{I}_q) \\prod^q_{i=1} (\\frac{\\alpha_i}{2\\pi})^{d/2} \\text{exp} \\big \\{ -\\frac{1}{2} \\alpha_i ||w_i||^2 \\big \\} \\Gamma(\\alpha_i|a_\\alpha, b_\\alpha) \\mathcal{N}(\\mu|\\mathbf{0}, \\beta^{-1}\\mathbf{I})  \\Gamma(\\tau|c_\\tau, d_\\tau)$\n",
      "\n",
      "Taking the log, using \n",
      "* $det(\\eta I_r) = \\eta^r$, \n",
      "* $(\\eta^{-1} I_r)^{-1} = \\eta I_r $, \n",
      "* $\\mathbf{f}- \\mathbf{0}= \\mathbf{f}$, \n",
      "* and $\\text{dimensionality}(\\mathbf{\\mu}) = d$\n",
      "\n",
      "We get:\n",
      "\n",
      "$\\text{ln }p(X,Z,W,\\alpha,\\tau,\\mu) = \\sum^N_{n=1} \\big [ \\text{ln } \\mathcal{N}(x_n|Wz_n + \\mu, \\tau\\mathbf{I}_d) + \\text{ln } \\mathcal{N}(z_n|\\mathbf{0}, \\mathbf{I}_q)\\big ] + \\sum^q_{i=1} \\text{ln } \\big [ (\\frac{\\alpha_i}{2\\pi})^{d/2} \\text{exp} \\big \\{ -\\frac{1}{2} \\alpha_i ||w_i||^2 \\big \\} + \\text{ln }\\Gamma(\\alpha_i|a_\\alpha, b_\\alpha)  \\big ] + \\text{ln } \\mathcal{N}(\\mu|\\mathbf{0}, \\beta^{-1}\\mathbf{I}) + \\text{ln } \\Gamma(\\tau|c_\\tau, d_\\tau)$\n",
      "\n",
      "$ = \\sum^N_{n=1} \\big [- \\frac{d}{2}\\text{ln }2\\pi - \\frac{d}{2} \\text{ln } \\tau - \\frac{1}{2}(x_n - Wz_n + \\mu)^T\\tau^{-1}\\mathbf{I}_d(x_n-Wz_n + \\mu) - \\frac{d}{2}\\text{ln }2\\pi - \\frac{1}{2}z_n^Tz_n \\big ] + \\sum^q_{i=1}  \\big [ -\\frac{d}{2}\\text{ln }(\\frac{\\alpha_i}{2\\pi})  -\\frac{1}{2} \\alpha_i ||w_i||^2 + a_\\alpha\\text{ln }b_\\alpha + (a_\\alpha-1)\\text{ln } \\alpha_i - b_\\alpha\\alpha_i -\\text{ln }\\Gamma(a_\\alpha) \\big ] -\\frac{d}{2} \\text{ln }2\\pi + \\frac{d}{2} \\text{ln } \\beta - \\frac{1}{2}\\mu^T\\beta I \\mu  + c_\\tau\\text{ln }d_\\tau + (c_\\tau-1)\\text{ln }\\tau - d_\\tau\\tau - \\text{ln }\\Gamma(c_\\tau)$\n",
      "\n",
      "\n",
      "Could we use this to assess the convergence of the variational Bayesian PCA algorithm? If yes, how? If no, why not?\n",
      "* To assess the convergence of the algorithm, we want to marginalize the joint $(X,Z,W,\\alpha,\\tau,\\mu)$ over the latent variables $Z, \\mu, \\tau$ and $\\alpha$ so we can evaluate the marginal log-likelihood p(X). However, this is computationally intractible. So in theory, yes, but in practice, no."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 4. The lower bound $\\mathcal{L}(Q)$ (25 points)\n",
      "\n",
      "Derive an expression for the lower bound $\\mathcal{L}(Q)$ of the log-prob $\\ln p(X)$ for Bayesian PCA, making use of the factorization (eq. 12) and the form of the Q-distributions (eq. 16-20) as listed in [Bishop99]. Show your steps. Implement this function.\n",
      "\n",
      "The following result may be useful:\n",
      "\n",
      "For $x \\sim \\Gamma(a,b)$, we have $\\langle \\ln x\\rangle = -\\ln b + \\psi(a)$, where $\\psi(a) = \\frac{\\Gamma'(a)}{\\Gamma(a)}$ is the digamma function (which is implemented in numpy.special)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From equation 10 we know that\n",
      "$\\mathcal{L}(Q) = \\int Q(\\Theta) \\text{ln } \\frac{P(D, \\Theta)}{Q(\\Theta)} d\\Theta$\n",
      "\n",
      "$= \\int Q(\\Theta) \\text{ln } P(D,\\Theta)d\\Theta - \\int Q(\\Theta) \\text{ln } Q(\\Theta) d\\Theta$\n",
      "\n",
      "$= \\int \\dotsb \\int Q(Z,W,\\mu,\\tau,\\alpha) \\text{ln } P(X,Z,W,\\mu,\\tau,\\alpha)dZd\\mu d\\tau d\\alpha dW - \\int \\dotsb \\int Q(Z,W,\\mu,\\tau,\\alpha) \\text{ln } Q(Z, W,\\mu,\\tau,\\alpha) dZ d\\mu d\\tau d\\alpha dW$\n",
      "\n",
      "$=\\int \\dotsb \\int Q(Z)q(W)Q(\\mu)Q(\\tau)Q(\\alpha)\\text{ln }p(X,Z,W,\\alpha,\\tau,\\mu)dZd\\mu d\\tau d\\alpha dW - \\int \\dotsb \\int Q(Z)Q(W)Q(\\mu)Q(\\tau)Q(\\alpha) \\text{ln} Q(Z)Q(W)Q(\\mu)Q(\\tau)Q(\\alpha)dZd\\mu d\\tau d\\alpha dW  $\n",
      "\n",
      "$=\\int \\dotsb \\int Q(Z)Q(W)Q(\\mu)Q(\\tau)Q(\\alpha) \\bigg ( \\big (\\sum^N_{n=1} \\text{ln } p(x_n|z_n, W, \\mu, \\tau) \\big) + \\text{ln }p(Z)+ \\text{ln }p(W|\\alpha) + \\text{ln } p(\\alpha) + \\text{ln } p(\\mu) + \\text{ln } p(\\tau) \\bigg) dZd\\mu d\\tau d\\alpha dW- \\int \\dotsb \\int Q(Z)Q(W)Q(\\mu)Q(\\tau)Q(\\alpha) \\big( \\text{ln }Q(Z) + \\text{ln }Q(W) + \\text{ln }Q(\\mu) + \\text{ln }Q(\\tau) + \\text{ln }Q(\\alpha) \\big)dZd\\mu d\\tau d\\alpha dW $ \n",
      "\n",
      "$ =\\int \\dotsb \\int Q(Z)Q(W)Q(\\mu)Q(\\tau)Q(\\alpha) \\big (\\sum^N_{n=1} \\text{ln } p(x_n|z_n, W, \\mu, \\tau) \\big) Zd\\mu d\\tau d\\alpha dW +\n",
      "\\int \\dotsb \\int Q(Z)Q(W)Q(\\mu)Q(\\tau)Q(\\alpha)\\text{ln }p(Z)dZd\\mu d\\tau d\\alpha dW  +\n",
      "\\int \\dotsb \\int Q(Z)Q(W)Q(\\mu)Q(\\tau)Q(\\alpha)\\text{ln }p(W|\\alpha) dZd\\mu d\\tau d\\alpha dW +\n",
      "\\int \\dotsb \\int Q(Z)Q(W)Q(\\mu)Q(\\tau)Q(\\alpha)\\text{ln } p(\\mu)dZd\\mu d\\tau d\\alpha dW + \n",
      "Q(Z)Q(W)Q(\\mu)Q(\\tau)Q(\\alpha)\\text{ln } p(\\tau)dZd\\mu d\\tau d\\alpha dW -\n",
      "\\int \\dotsb \\int Q(Z)Q(W)Q(\\mu)Q(\\tau)Q(\\alpha) \\text{ln }Q(Z)dZd\\mu d\\tau d\\alpha dW - \n",
      "\\int \\dotsb \\int Q(Z)Q(W)Q(\\mu)Q(\\tau)Q(\\alpha) \\text{ln }Q(W)dZd\\mu d\\tau d\\alpha dW - \n",
      "\\int \\dotsb \\int Q(Z)Q(W)Q(\\mu)Q(\\tau)Q(\\alpha)\\text{ln }Q(\\mu)dZd\\mu d\\tau d\\alpha dW - \n",
      "\\int \\dotsb \\int Q(Z)Q(W)Q(\\mu)Q(\\tau)Q(\\alpha)\\text{ln }Q(\\tau)dZd\\mu d\\tau d\\alpha dW - \n",
      "\\int \\dotsb \\int Q(Z)Q(W)Q(\\mu)Q(\\tau)Q(\\alpha)\\text{ln }Q(\\alpha)dZd\\mu d\\tau d\\alpha dW $\n",
      "\n",
      "\n",
      "$ =\\int \\dotsb \\int Q(Z)Q(W)Q(\\mu)Q(\\tau) \\big (\\sum^N_{n=1} \\text{ln } p(x_n|z_n, W, \\mu, \\tau) \\big) Zd\\mu d\\tau dW +  \\int Q(Z)\\text{ln }p(Z)dZ  + \\int Q(W)\\text{ln }p(W|\\alpha) dW + \\int Q(\\mu)\\text{ln } p(\\mu)d\\mu  + \\int Q(\\tau)\\text{ln } p(\\tau)d\\tau\n",
      "- \\int Q(Z)\\text{ln }Q(Z)dZ- \\int Q(W)\\text{ln }Q(W)dW - \\int Q(\\mu)\\text{ln }Q(\\mu)d\\mu - \\int Q(\\tau)\\text{ln }Q(\\tau)d\\tau - \\int  Q(\\alpha)\\text{ln }Q(\\alpha)d\\alpha$\n",
      "\n",
      "$ = \\{\\sum^N_{n=1} \\text{ln } p(x_n|z_n, W, \\mu, \\tau) \\} +  \\mathbb{E}_{Q(Z)}[\\text{ln }p(Z)] + \\mathbb{E}_{Q(W)}[\\text{ln }p(W|\\alpha)] + \\mathbb{E}_{Q(\\mu)}[\\text{ln } p(\\mu)] + \\mathbb{E}_{Q(\\tau)}[\\text{ln} p(\\tau)]\n",
      "+ H[Q(Z)] + H[Q(W)] + H[Q(\\mu)] + H[Q(\\tau)] + H[Q(\\alpha)]$\n",
      "\n",
      "Using definitions of Bishop (p513) for parameters in Q distributions (due to space):\n",
      "\n",
      "$ = \\{\\sum^N_{n=1} \\text{ln } \\mathcal{N}(x_n|Wz_n + \\mu, \\tau\\mathbf{I}_d) \\} + \\mathbb{E}_{Q(Z)}[\\text{ln }\\prod^N_{n=1} \\mathcal{N}(z_n|\\mathbf{0}, \\mathbf{I}_q)] + \\mathbb{E}_{Q(W)}[\\text{ln } \\prod^q_{i=1} (\\frac{\\alpha_i}{2\\pi})^{d/2} \\text{exp} \\big \\{ -\\frac{1}{2} \\alpha_i ||w_i||^2 \\big \\}] + \\mathbb{E}_{Q(\\mu)}[\\text{ln } \\mathcal{N}(\\mu|\\mathbf{0}, \\beta^{-1}\\mathbf{I})] + \\mathbb{E}_{Q(\\tau)}[\\text{ln} \\Gamma(\\tau|c_\\tau, d_\\tau)] +  H[Q(Z)] + H[\\prod\\limits_{k=1}^d \\mathcal{N}(\\tilde{\\mathbf{w}}_k|\\mathbf{m}_{w}^{(k)}, \\boldsymbol\\Sigma_{\\mathbf{w}})] + H[\\mathcal{N}(\\boldsymbol\\mu|\\mathbf{m}_\\mu, \\boldsymbol\\Sigma_{\\boldsymbol\\mu})] + H[\\Gamma(\\tau|\\tilde{a_{\\tau}}, \\tilde{b_{\\tau}})] + H[\\prod\\limits_{i=1}^q\\Gamma(\\alpha_i|\\tilde{a_{\\alpha}}, \\tilde{b_{\\alpha i}})]$\n",
      "\n",
      "= $ \\{\\sum^N_{n=1} \\big [- \\frac{d}{2}\\text{ln }2\\pi - \\frac{d}{2} \\text{ln } \\tau - \\frac{1}{2}(x_n - Wz_n + \\mu)^T\\tau^{-1}\\mathbf{I}_d(x_n-Wz_n + \\mu)\\big ] + \\mathbb{E}_{Q(Z)}[- \\frac{d}{2}\\text{ln }2\\pi - \\frac{1}{2}z_n^Tz_n ] +  \\mathbb{E}_{Q(W)}[\\sum^q_{i=1}  \\big [ -\\frac{d}{2}\\text{ln }(\\frac{\\alpha_i}{2\\pi})  -\\frac{1}{2} \\alpha_i ||w_i||^2] +  \\mathbb{E}_{Q(\\mu)}[-\\frac{d}{2} \\text{ln }2\\pi + \\frac{d}{2} \\text{ln } \\beta - \\frac{1}{2}\\mu^T\\beta I \\mu] + \\mathbb{E}_{Q(\\tau)}[c_\\tau\\text{ln }d_\\tau + (c_\\tau-1)\\text{ln }\\tau - d_\\tau\\tau - \\text{ln }\\Gamma(c_\\tau)]\n",
      "+ H[Q(Z)] + H[\\prod\\limits_{k=1}^d \\mathcal{N}(\\tilde{\\mathbf{w}}_k|\\mathbf{m}_{w}^{(k)}, \\boldsymbol\\Sigma_{\\mathbf{w}})] + H[\\mathcal{N}(\\boldsymbol\\mu|\\mathbf{m}_\\mu, \\boldsymbol\\Sigma_{\\boldsymbol\\mu})] + H[\\Gamma(\\tau|\\tilde{a_{\\tau}}, \\tilde{b_{\\tau}})] + H[\\prod\\limits_{i=1}^q\\Gamma(\\alpha_i|\\tilde{a_{\\alpha}}, \\tilde{b_{\\alpha i}})]$\n",
      "\n",
      "= $- \\frac{dN}{2}\\text{ln }2\\pi - \\frac{dN}{2} \\text{ln } \\tau - \\sum^N_{n=1}\\{\\frac{1}{2}(x_n - Wz_n + \\mu)^T\\tau^{-1}\\mathbf{I}_d(x_n-Wz_n + \\mu)\\} -\\frac{dN}{2}\\text{ln }2\\pi - \\sum^N_{n=1}\\mathbb{E}_{Q(Z)}[\\frac{1}{2}z_n^Tz_n ] \n",
      "- \\sum^q_{i=1} \\frac{d}{2}\\text{ln }(\\frac{\\alpha_i}{2\\pi}) -  \\sum^q_{i=1} \\mathbb{E}_{Q(W)}[-\\frac{1}{2} \\alpha_i ||w_i||^2]\\}\n",
      "-\\frac{d}{2} \\text{ln }2\\pi + \\frac{d}{2} \\text{ln } \\beta - \\mathbb{E}_{Q(\\mu)}[\\frac{1}{2}\\mu^T\\beta I \\mu]\n",
      "+ H[Q(Z)] + H[\\prod\\limits_{k=1}^d \\mathcal{N}(\\tilde{\\mathbf{w}}_k|\\mathbf{m}_{w}^{(k)}, \\boldsymbol\\Sigma_{\\mathbf{w}})] + H[\\mathcal{N}(\\boldsymbol\\mu|\\mathbf{m}_\\mu, \\boldsymbol\\Sigma_{\\boldsymbol\\mu})] + H[\\Gamma(\\tau|\\tilde{a_{\\tau}}, \\tilde{b_{\\tau}})] + H[\\prod\\limits_{i=1}^q\\Gamma(\\alpha_i|\\tilde{a_{\\alpha}}, \\tilde{b_{\\alpha i}})]\n",
      "$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}